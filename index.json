[{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.1-blog1/","title":"AWS CDK Launches Refactoring Feature","tags":[],"description":"","content":"AWS Cloud Development Kit (CDK) Launches Refactoring Feature We are excited to introduce a new AWS Cloud Development Kit (CDK) feature that makes refactoring infrastructure as code easier and safer. CDK Refactor aims to preserve AWS resources when you rename constructs, move resources between stacks, and reorganize CDK applications ‚Äì operations that previously risked resource replacement.\nThe Problem When writing infrastructure as code with CDK, developers sometimes need to rename Constructs or move them between Stacks or directories. Whether they need to better organize code, follow coding best practices, or leverage object-oriented programming patterns like class inheritance, these changes can pose risks in environments with deployed resources, as they alter the logical IDs that CDK generates for those resources.\nDuring CDK deployment, AWS CloudFormation interprets these changes as new resources, often requiring deletion of existing resources and creation of new resources with new logical IDs. For stateful resources, this can cause potential downtime and even data loss.\nTo mitigate the impact of these ID changes, developers must stage their changes to create new resources, create data or network migration plans, then delete old resources to prevent these refactoring impacts. Sometimes, developers decide that the risk of these changes outweighs the benefits of refactoring and choose not to refactor.\nThe Solution Today, developers can use the new CDK refactor command to detect, review, validate, and safely apply refactored changes to their resources without replacing resources. This feature leverages the recently launched AWS CloudFormation refactoring capability, but CDK automatically calculates the mappings that CloudFormation needs to reidentify refactored resources, providing an abstraction layer that allows developers to focus on code rather than resource configuration.\nPrerequisites Along with the usual CDK prerequisites, if you bootstrapped your CDK project before this launch, you need to re-bootstrap your environment to get the new permissions related to CDK refactoring capability before attempting to refactor.\nExample: From Monolith to Microservices In this example, suppose we have a legacy CDK application deploying a monolithic Stack with Amazon DynamoDB tables for users, products, and orders, along with a single AWS Lambda function implementing CRUD operations on all entities.\nInitial Architecture (Monolith) function monolithApp() { const monolith = new CdkAppStack(app, monolithStackName, {env}); const usersTable = makeTable(monolith, \u0026#39;users\u0026#39;); const productsTable = makeTable(monolith, \u0026#39;products\u0026#39;); const ordersTable = makeTable(monolith, \u0026#39;orders\u0026#39;); // We have a single Lambda function in our application const func = new Function(monolith, `MonolithFunction`, { code: Code.fromInline(`Some code that accesses all three tables`), runtime: Runtime.NODEJS_22_X, handler: \u0026#39;index.handler\u0026#39;, }); usersTable.grantReadWriteData(func); productsTable.grantReadWriteData(func); ordersTable.grantReadWriteData(func); // This function creates a REST API, resources, methods, and links // everything together to the functions. Right now, we are passing // the same function in three places. makeApi(monolith, { usersFunction: func, productsFunction: func, ordersFunction: func, }); } monolithApp(); Refactoring Requirements We are asked to comply with Well Architected Framework best practices and split the monolith into separate Lambda functions so they can scale independently. Since they are very similar, we will also create an inheritable Lambda class that we can reuse to improve code readability and maintainability, while avoiding having to redefine uniform Lambda configuration settings across all functions.\nFinally, the monolith only uses CDK L1 Constructs. To further abstract the code and leverage helper functions, we will start using CDK L2 Constructs for DynamoDB, Lambda, and API Gateway. This change will allow automatic definition of IAM Permissions and Roles, simplifying the code.\nThe proposed application is refactored into separate stacks for each domain. Without the refactoring feature, CloudFormation would delete and recreate resources, causing data loss.\nReplacing Stateless Resources Refactor CDK code to split the monolithic Lambda into 3 separate Lambdas:\nfunction singleStackMicroservicesApp() { const monolith = new CdkAppStack(app, monolithStackName, {env}); makeApi(monolith, { usersFunction: makeFunctionAndTable(monolith, \u0026#39;users\u0026#39;), productsFunction: makeFunctionAndTable(monolith, \u0026#39;products\u0026#39;), ordersFunction: makeFunctionAndTable(monolith, \u0026#39;orders\u0026#39;), }); } Refactoring Stateful Resources Create new stacks and move resources:\nfunction fullMicroservicesApp() { const monolith = new Stack(app, monolithStackName, {env}); const usersStack = new Stack(app, \u0026#39;Users\u0026#39;, {env}); const productsStack = new Stack(app, \u0026#39;Products\u0026#39;, {env}); const ordersStack = new Stack(app, \u0026#39;Orders\u0026#39;, {env}); makeApi(monolith, { usersFunction: makeFunctionAndTable(usersStack, \u0026#39;users\u0026#39;), productsFunction: makeFunctionAndTable(productsStack, \u0026#39;products\u0026#39;), ordersFunction: makeFunctionAndTable(ordersStack, \u0026#39;orders\u0026#39;), }); } Run cdk refactor --unstable=refactor to safely move resources without data loss.\nConclusion The CDK Refactor feature allows developers to confidently refactor infrastructure code, preserve AWS resources, and minimize risk. Read more in the CDK Refactor documentation to get started!\nSource: AWS Blog\nDate: September 10, 2025\nTranslated by: Duong Nguyen Gia Huy\n"},{"uri":"https://thienluhoan.github.io/workshop-template/","title":"Internship Report","tags":[],"description":"","content":"Internship Report Student Information: Full Name: Duong Nguyen Gia Huy\nPhone Number: 0939007645\nEmail: huydngse182202@fpt.edu.vn\nUniversity: FPT University\nMajor: AI\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.1-event1/","title":"AWS Cloud Mastery Series #1","tags":[],"description":"","content":"AWS Cloud Mastery Series #1 AI/ML/GenAI on AWS Event Details üìÖ Saturday, November 15, 2025\nüï£ 8:30 AM ‚Äì 12:00 PM\nüìç Bitexco Financial Tower\nQu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh\nAgenda 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; Introduction Participant registration and networking Workshop overview and learning objectives Ice-breaker activity Overview of the AI/ML landscape in Vietnam 9:00 ‚Äì 10:30 AM | AWS AI/ML Services Overview Amazon SageMaker ‚Äì End-to-end ML platform\nData preparation and labeling Model training, tuning, and deployment Integrated MLOps capabilities Live Demo: SageMaker Studio walkthrough 10:30 ‚Äì 10:45 AM | Coffee Break 10:45 AM ‚Äì 12:00 PM | Generative AI with Amazon Bedrock Foundation Models: Claude, Llama, Titan ‚Äì comparison \u0026amp; selection guide Prompt Engineering: Techniques, Chain-of-Thought reasoning, Few-shot learning Retrieval-Augmented Generation (RAG): Architecture \u0026amp; Knowledge Base integration Bedrock Agents: Multi-step workflows and tool integrations Guardrails: Safety and content filtering Live Demo: Building a Generative AI chatbot using Bedrock Key Takeaways Hands-on experience with AWS AI/ML services Understanding of Generative AI capabilities with Amazon Bedrock Practical knowledge of RAG architecture and implementation Networking with AI/ML practitioners in Vietnam "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.1-workshop-overview/","title":"Introduction","tags":[],"description":"","content":"Amazon QuickSight Amazon QuickSight is a cloud-scale business intelligence (BI) service that you can use to deliver easy-to-understand insights to the people who you work with, wherever they are. QuickSight connects to your data in the cloud and combines data from many different sources. In a single data dashboard, QuickSight can include AWS data, third-party data, big data, spreadsheet data, SaaS data, B2B data, and more. Key Features \u0026ldquo;SPICE: \u0026ldquo; The Super-fast, Parallel, In-memory Calculation Engine. SPICE is engineered to rapidly perform advanced calculations and serve data. \u0026ldquo;Auto-Graph\u0026rdquo; Automatically selects the best visualization for your data. \u0026ldquo;ML Insights: \u0026ldquo; Leverages machine learning to uncover hidden trends and outliers. \u0026ldquo;Embedded Analytics: \u0026ldquo; Embed dashboards into your applications. Workshop Scenario In this workshop, you will act as a Data Analyst for a retail company. You have been given a dataset of sales records and tasked with creating a dashboard to visualize sales performance by region, date, and product category.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 09/08/2025 09/08/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 09/09/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic budget types: + Cost budget + Usage budget + Saving plans budget + Reservation budget 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn and practice: + AWS support packages + Change AWS support packages + Manage support request 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute: Used to run applications, process data, create virtual server environments or containers. Main services: EC2 (Elastic Compute Cloud): Virtual servers running on AWS (like VPS but more flexible). Lambda: Run serverless code, no server management needed, pay per execution. ECS/EKS (Elastic Container Service / Elastic Kubernetes Service): Manage containers (Docker/K8s). Elastic Beanstalk: Automatic app deployment (like PaaS)\nStorage: Store data, from small files to Big Data. S3 (Simple Storage Service): Object storage ‚Äì used for files, images, videos. EBS (Elastic Block Store): Block storage (used with EC2 like hard drives). EFS (Elastic File System): File system storage, multiple machines can access simultaneously. Glacier: Long-term storage, cheap (for backup/archive).\nNetworking: Connect services, secure and distribute applications. VPC (Virtual Private Cloud): Create private virtual network in AWS (like private data center). Route 53: DNS service, domain management. CloudFront: CDN distributes content faster to global users. ELB (Elastic Load Balancer): Load balancing between multiple servers. API Gateway: Manage and secure APIs.\nDatabase: Store and manage structured/unstructured data. RDS (Relational Database Service): Manage relational databases (MySQL, PostgreSQL, Oracle, SQL Server). Aurora: AWS-developed database, MySQL/PostgreSQL compatible, faster than RDS. DynamoDB: NoSQL Database (non-relational), high speed, auto-scaling. Redshift: Data warehouse for big data analysis. ElastiCache: Data caching (Redis/Memcached).\nSuccessfully created and configured AWS Free Tier account.\nBecame familiar with AWS Management Console and learned how to find, access, and use services via web interface.\nInstalled and configured AWS CLI on computer including:\nAccess Key Secret Key Default Region Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve list of regions View EC2 service Create and manage key pairs Check information about running services "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/","title":"Worklog","tags":[],"description":"","content":"On this page, you will find my worklog documenting my AWS learning journey. I completed this program over 12 weeks (approximately 3 months), systematically learning and practicing AWS services from fundamentals to advanced topics.\nThroughout this internship period, I progressed from basic AWS concepts to designing and implementing complete cloud architectures. Each week focused on specific AWS services and hands-on practice, building upon previous knowledge to develop comprehensive cloud computing skills.\nBelow is my weekly learning progression:\nWeek 1: Getting familiar with AWS and basic AWS services\nWeek 2: Learning AWS Virtual Private Cloud (VPC)\nWeek 3: Learning Amazon EC2 and compute services\nWeek 4: Understanding AWS IAM and EC2 Instance Storage\nWeek 5: Learning High Availability, Scalability, and Database services\nWeek 6: Learning Amazon Route 53 and Classic Solutions Architecture\nWeek 7: Understanding Amazon S3 and storage features\nWeek 8: Learning CloudFront, Global Accelerator, and AWS Integration \u0026amp; Messaging\nWeek 9: Learning Containers and Serverless architectures\nWeek 10: Understanding Databases, Data \u0026amp; Analytics, and Machine Learning services\nWeek 11: Learning AWS Monitoring, Security, and Advanced Identity\nWeek 12: Learning Disaster Recovery, Migration strategies, and comprehensive review\n"},{"uri":"https://thienluhoan.github.io/workshop-template/2-proposal/","title":"Proposal","tags":[],"description":"","content":"Document QA with AWS Bedrock Intelligent Document Analysis System using RAG 1. Executive Summary The Document QA system is a serverless application designed to revolutionize how users interact with documents. By leveraging AWS Bedrock for Generative AI and RAG (Retrieval Augmented Generation) technology, the platform allows users to upload PDF/TXT documents and ask natural language questions. The system provides accurate, context-aware answers by retrieving relevant information from the uploaded documents, significantly reducing manual search time and improving information accessibility.\n2. Problem Statement What\u0026rsquo;s the Problem? Traditional document search methods (keyword matching) often fail to capture context or semantic meaning. Manual document review is time-consuming, error-prone, and inefficient, especially for large volumes of text. Users struggle to extract specific insights quickly, leading to productivity bottlenecks.\nThe Solution We propose a Serverless RAG-based Chatbot using AWS Bedrock (Amazon Titan). The solution involves:\nUpload \u0026amp; Processing: Users upload documents to S3; Lambda functions trigger text extraction and embedding generation. Vector Search: Embeddings are stored and queried to find relevant document chunks. Generative AI: AWS Bedrock generates natural language responses based on the retrieved context. Serverless Architecture: Built on AWS Lambda, API Gateway, and DynamoDB for automatic scaling and cost efficiency. Benefits and Return on Investment Efficiency: Reduces document analysis time from hours to seconds. Accuracy: RAG ensures answers are grounded in the provided document, minimizing hallucinations. Cost-Effective: Serverless pay-as-you-go model (estimated \u0026lt; $5/month for low usage). Scalability: Automatically handles varying loads without manual infrastructure management. 3. Solution Architecture The platform employs a modern serverless architecture to ensure scalability, security, and performance.\nAWS Services Used AWS Bedrock: Provides the Foundation Models (Amazon Titan) for embeddings and text generation. AWS Lambda: Serverless compute for handling API requests, document processing, and orchestration. Amazon API Gateway: Manages REST API endpoints for the frontend. Amazon S3: Stores raw uploaded documents and frontend static assets. Amazon DynamoDB: Manages user sessions and chat history. Vector Store: (Implemented via Lambda/Local or dedicated vector DB) Stores document embeddings for semantic search. Component Design Frontend: Hosted on S3 (or Amplify), providing a user-friendly chat interface. API Layer: API Gateway routes requests (/upload, /ask) to Lambda functions. Processing Layer: Lambda handles text extraction, calls Bedrock for embeddings, and performs vector similarity search. AI Layer: AWS Bedrock generates responses using the retrieved context and user query. 4. Technical Implementation Implementation Phases\nPhase 1: Foundation (Weeks 1-4): Setup AWS environment, Bedrock access, and basic backend logic. Phase 2: API \u0026amp; Security (Weeks 5-7): Develop API Gateway, Lambda functions, and implement CORS/Security. Phase 3: Frontend Development (Weeks 8-11): Build the React/Next.js interface and integrate with APIs. Phase 4: Testing \u0026amp; Deployment (Weeks 12-14): End-to-end testing, optimization, and final deployment. Technical Requirements\nAI Model: Amazon Titan (via Bedrock) for Embeddings and Text Generation. Backend: Node.js/Python on AWS Lambda. Infrastructure as Code: Serverless Framework or AWS CDK. Frontend: React.js / Next.js. 5. Timeline \u0026amp; Milestones Month 1: Architecture Design, AWS Setup, Backend Core (Upload/Embeddings). Month 2: RAG Implementation, Vector Search Logic, API Development. Month 3: Frontend Integration, UI/UX Polish, Testing, and Launch. 6. Budget Estimation Estimated Monthly Costs (Low-Medium Usage)\nAWS Bedrock (Titan): ~$0 (Free Tier / Low cost per 1k tokens) AWS Lambda: ~$0.20 per 1M requests Amazon S3: ~$0.023 per GB Amazon DynamoDB: ~$0.25 per 1M requests Amazon API Gateway: ~$3.50 per 1M requests Total Estimated: \u0026lt; $5.00 / month\n7. Risk Assessment Risk Matrix Hallucinations (AI Errors): Medium Impact, Medium Probability. Cost Overruns: Medium Impact, Low Probability (Serverless). Data Leakage: High Impact, Low Probability. Mitigation Strategies Hallucinations: Strict RAG implementation (grounding answers in context). Cost: Set AWS Budget Alerts and usage quotas. Security: Use Presigned URLs for S3, IAM roles with least privilege. 8. Expected Outcomes Technical Improvements Fully automated document analysis pipeline. Sub-second retrieval latency for vector search. Scalable architecture supporting concurrent users. Long-term Value A reusable RAG framework for future knowledge base applications. Significant productivity gains for users needing quick information retrieval. 9. Team Structure and Responsibilities Name Student ID Primary Role Email/Contact Info Nguy·ªÖn L√™ Anh Qu√¢n SE192307 Team Leader/ Cloud Architect nguyenleanhquan2005@gmail.com ƒê√†o Quang Vinh SE180012 Project Manager/ Backend Developer (Bedrock, RAG) its.vnhdq@gmail.com Nguy·ªÖn Thanh Li√™m SE184163 Backend Developer liemntse184163@fpt.edu.vn Tr·∫ßn ƒê√¨nh Phong SE184217 Frontend Developer/ UI/UX Designer phongtdse184217@fpt.edu.vn D∆∞∆°ng Nguy·ªÖn Gia Huy SE182202 QA Engineer/Backend Developer (Bedrock, RAG) huydngse182202@fpt.edu.vn Detailed Responsibilities by Team Member Nguy·ªÖn L√™ Anh Qu√¢n - Cloud Architect/ Team Leader Primary Responsibilities:\nAWS architecture design and service selection Infrastructure planning and optimization Security architecture and IAM policies Technical consultation and best practices ƒê√†o Quang Vinh - Project Manager/Backend Developer Primary Responsibilities:\nOverall project management and timeline coordination Team coordination and task assignment Progress reporting to instructor/advisor Risk management and mitigation strategies Documentation oversight and quality assurance Build vector search and retrieval logic Develop chat/query handler Lambda function D∆∞∆°ng Nguy·ªÖn Gia Huy - QA Engineer/Backend Developer Primary Responsibilities:\nCore backend logic development Amazon Bedrock integration (Foundation Models) RAG (Retrieval-Augmented Generation) pipeline implementation Develop Lambda function for document ingestion Integrate Amazon Bedrock Knowledge Bases Implement text chunking and embedding generation Nguy·ªÖn Thanh Li√™m - Backend Developer Primary Responsibilities:\nBackend infrastructure and data management CI/CD pipeline development System monitoring and logging Performance optimization DynamoDB schema design and implementation Conversation history storage logic Tr·∫ßn ƒê√¨nh Phong - Frontend Developer Primary Responsibilities:\nUser interface design and development Frontend-backend integration User experience optimization Responsive design implementation Implement file upload interface with drag-and-drop Connect frontend to API Gateway endpoints Handle API responses and error states Deploy frontend to S3 + CloudFront "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.2-blog2/","title":"Modern Observability Frameworks for SAP","tags":[],"description":"","content":"Modern Observability Frameworks for SAP: Solutions for Monitoring Challenges with PowerConnect and Dynatrace This post was co-authored with Dynatrace and SoftwareOne PowerConnect. We sincerely thank Krzysztof Ziemianowicz, Director of Extension Services at Dynatrace, and Stephen Bangs, SAP Solutions Architect at SoftwareOne PowerConnect, for their contributions and support.\nThe Challenge Modern SAP environments require sophisticated monitoring capabilities as business processes extend beyond the scope of individual systems. Organizations are now managing interconnected platforms, including:\nSAP Cloud ERP SAP Business Technology Platform (BTP) AWS services Multiple other cloud solutions This complexity demands a new approach to system monitoring. Traditional monitoring tools, designed for single-system metrics and reactive alerts, no longer meet the needs of today\u0026rsquo;s integrated business processes. Modern operations require comprehensive visibility across all connected systems to maintain optimal performance.\nThe Solution: PowerConnect and Dynatrace SoftwareOne PowerConnect for SAP Solutions addresses these challenges by providing comprehensive coverage across the SAP ecosystem. Its observability framework transforms system-specific monitoring operations into actionable intelligence through OpenTelemetry standards, enabling proactive performance management and providing real-time operational insights.\nBy combining PowerConnect\u0026rsquo;s deep SAP expertise with Dynatrace\u0026rsquo;s AI-powered platform, organizations achieve:\nComprehensive system visibility Proactive issue detection Automated root cause analysis Improved business outcomes This integrated approach provides comprehensive process visibility, helping organizations prevent disruptions and optimize operations across the entire SAP landscape.\nEnhancing SAP Visibility PowerConnect collects observability signals from SAP Cloud ERP, SAP BTP, and SAP SaaS, then records them in the Dynatrace Grail data store. This is where organizations can maintain all AWS observability signals under a unified analytics model ‚Äì exactly as you view your entire IT environment as a unified entity.\nDynatrace is part of the AWS Agentic AI Marketplace, and Dynatrace solutions can now be pre-integrated with AWS AI services, such as:\nAmazon Q Amazon Bedrock Amazon SageMaker This collaboration enables businesses to extract deeper insights and confidently drive digital transformation.\nPlatform Capabilities The AI-powered Dynatrace observability platform transforms telemetry data into actionable intelligence through:\nSophisticated AI/ML-powered pattern detection Automated analysis Pre-built dashboards Visual visualization of complex SAP environments Rapid identification of performance issues Discovery of optimization opportunities The platform\u0026rsquo;s comprehensive AI-powered contextual intelligence ensures organizations can track transactions and understand dependencies across their entire technology stack. PowerConnect Installation PowerConnect installation for SAP solutions is supported across all SAP systems in SAP Cloud ERP private. Installation steps:\nSubmit request to SAP Enterprise Cloud Services (ECS) to install PowerConnect ABAP (including SAP S/4HANA) and if needed, install PowerConnect Java (including SAP Process Orchestration and SAP BusinessObjects)\nSubmit request to SAP ECS to allow Dynatrace tenant URL from proxy server in SAP managed account\nInstall PowerConnect for SAP on Dynatrace application from Dynatrace Hub\nVisualize and analyze SAP signals using Dynatrace dashboards when data begins streaming\nUse Cases After configuration, organizations can leverage pre-built dashboards for use cases:\nBackground and batch jobs ABAP dumps and runtime errors Security audit log and application log analysis Workflows, locks, and update requests Transactional RFC and queued RFC transactions IDoc status, IDoc segments, and outbound distribution monitoring There are over 200 pre-designed dashboards in the PowerConnect for SAP on Dynatrace application, and organizations can build their own custom dashboards. Advanced Features Distributed Tracing application centralizes and analyzes SAP traces collected by PowerConnect across multiple systems.\nBusiness Flow application maps and evaluates SAP process execution and business events.\nA Single Solution for All SAP Environments The standard integration architecture supports SAP cloud solutions, including:\nSAP BTP SAP Integration Suite SAP SuccessFactors SAP Ariba SAP Fieldglass Deployment This architectural model is deployed as follows:\nProvision Amazon EC2: Organizations provision Amazon EC2 instances on AWS to host the PowerConnect Cloud standalone agent\nHigh Availability: The agent can be installed on one EC2 instance or two EC2 instances to provide high availability\nAPI Connection: The Amazon EC2 instance connects to SAP APIs and extracts signals forwarded to Dynatrace\nInstallation Documentation: PowerConnect Cloud installation steps on Amazon EC2 are provided in the installation documentation\nConnectivity: The EC2 instance needs to connect to:\nSAP BTP platform SAP SaaS applications Dynatrace tenant Cloud Foundry Option: PowerConnect Cloud can be installed in SAP BTP Cloud Foundry environment using Cloud Foundry command-line tools\nRole-based Dashboards Dynatrace provides a single data source that can instantly display necessary data in role-based dashboards, such as:\nSAP CPI message monitoring SAP BTP syslog insights Conclusion The integration of Dynatrace and PowerConnect on AWS delivers this modern observability framework, moving beyond traditional single-system approaches to provide the comprehensive visibility and AI-driven insights necessary for today\u0026rsquo;s connected SAP environments.\nKey Benefits By leveraging:\nAWS\u0026rsquo;s scalable infrastructure Native AI service integration through AWS Agentic AI Marketplace Seamless marketplace deployment options Organizations can rapidly deploy this solution to transform their SAP monitoring capabilities.\nBusiness Value The combination of Dynatrace and PowerConnect on AWS delivers tangible business value as organizations:\nGain real-time insights into critical business processes Identify bottlenecks Optimize workflows Enhance user experience across their entire SAP landscape Availability Dynatrace Platform and PowerConnect for SAP Solutions are now available on AWS Marketplace.\nSource: AWS Blog\nAuthors: Abhik Ray and Bharat Ramaka\nDate: September 9, 2025\nTranslated by: Duong Nguyen Gia Huy\nDynatrace, Grail, and the Dynatrace logo are trademarks of Dynatrace, Inc.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.2-event2/","title":"AWS Cloud Mastery Series #2","tags":[],"description":"","content":"AWS Cloud Mastery Series #2 DevOps on AWS Event Details üìÖ Monday, November 17, 2025\nüï£ 8:30 AM ‚Äì 5:00 PM\nüìç Bitexco Financial Tower\nQu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh\nMorning Session (8:30 AM ‚Äì 12:00 PM) 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; DevOps Mindset Recap of AI/ML session DevOps culture and principles Benefits and key metrics (DORA, MTTR, deployment frequency) 9:00 ‚Äì 10:30 AM | AWS DevOps Services ‚Äì CI/CD Pipeline Source Control: AWS CodeCommit, Git strategies (GitFlow, Trunk-based) Build \u0026amp; Test: CodeBuild configuration, testing pipelines Deployment: CodeDeploy with Blue/Green, Canary, and Rolling updates Orchestration: CodePipeline automation Demo: Full CI/CD pipeline walkthrough 10:30 ‚Äì 10:45 AM | Break 10:45 AM ‚Äì 12:00 PM | Infrastructure as Code (IaC) AWS CloudFormation: Templates, stacks, and drift detection AWS CDK (Cloud Development Kit): Constructs, reusable patterns, and language support Demo: Deploying with CloudFormation and CDK Discussion: Choosing between IaC tools Lunch Break (12:00 ‚Äì 1:00 PM) (Self-arranged)\nAfternoon Session (1:00 ‚Äì 5:00 PM) 1:00 ‚Äì 2:30 PM | Container Services on AWS Docker Fundamentals: Microservices and containerization Amazon ECR: Image storage, scanning, lifecycle policies Amazon ECS \u0026amp; EKS: Deployment strategies, scaling, and orchestration AWS App Runner: Simplified container deployment Demo \u0026amp; Case Study: Microservices deployment comparison 2:30 ‚Äì 2:45 PM | Break 2:45 ‚Äì 4:00 PM | Monitoring \u0026amp; Observability CloudWatch: Metrics, logs, alarms, and dashboards AWS X-Ray: Distributed tracing and performance insights Demo: Full-stack observability setup Best Practices: Alerting, dashboards, and on-call processes 4:00 ‚Äì 4:45 PM | DevOps Best Practices \u0026amp; Case Studies Deployment strategies: Feature flags, A/B testing Automated testing and CI/CD integration Incident management and postmortems Case Studies: Startups and enterprise DevOps transformations 4:45 ‚Äì 5:00 PM | Q\u0026amp;A \u0026amp; Wrap-up DevOps career pathways AWS certification roadmap Key Takeaways Complete understanding of AWS DevOps services and CI/CD pipelines Hands-on experience with Infrastructure as Code (CloudFormation \u0026amp; CDK) Container orchestration with ECS and EKS Monitoring and observability best practices Real-world DevOps case studies and implementation strategies "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.2-prerequiste/","title":"Prerequisites &amp; Data","tags":[],"description":"","content":"Step 1: Sign up for Amazon QuickSight Log in to the AWS Management Console. Search for QuickSight. If you have not signed up yet, you will be prompted to do so. Choose Sign up for QuickSight. Select the Enterprise edition (it offers a free trial). Follow the on-screen instructions: Authentication method: Use IAM federated identity \u0026amp; QuickSight-managed users (default). Region: Select US East (N. Virginia). QuickSight account name: Enter a unique name. Notification email: Enter your email. Allow access: You can leave the defaults or uncheck them if we are only uploading a file. For this workshop, we don\u0026rsquo;t strictly need S3 access, but it\u0026rsquo;s good practice to leave S3 checked if you plan to use it later. Click Finish. Step 2: Prepare Sample Data We will use a simple CSV file for this workshop.\nCopy the following data and save it as a file named sales_data.csv on your computer: Date,Region,Product,Sales,Quantity 2023-01-01,North,Laptop,1200,2 2023-01-02,South,Phone,800,5 2023-01-03,East,Tablet,400,3 2023-01-04,West,Laptop,1200,1 2023-01-05,North,Phone,800,2 2023-01-06,South,Tablet,400,4 2023-01-07,East,Laptop,1200,3 2023-01-08,West,Phone,800,6 2023-01-09,North,Tablet,400,2 2023-01-10,South,Laptop,1200,4 Alternatively, you can use any CSV file you have, but the instructions will follow this structure.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Learn about AWS Virtual Private Cloud Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Go to office and meet new friends - Learn basics about Amazon VPC - Theory about subnets, Route tables, Internet Gateway, NAT gateway 09/15/2025 09/15/2025 3 - Theory about firewalls in VPC - Security groups, Network ACLs, VPC Resource map 09/16/2025 09/16/2025 https://cloudjourney.awsstudygroup.com/ 4 - Do basic practice - Create VPC, Internet Gateway, Create route table, Create security group - Enable VPC flow Logs 09/17/2025 09/17/2025 https://cloudjourney.awsstudygroup.com/ 5 - Deploy Amazon EC2 + Create EC2 server + Create NAT Gateway + Use Reachability Analyzer - SSH connection methods to EC2 - Learn about Elastic IP 09/18/2025 09/18/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn: + Configure Site to Site VPN + Clean up resources 09/19/2025 09/19/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understood AWS and mastered the basic service groups of Amazon VPC: Subnets Route tables Internet gateway NAT gateway Firewalls in Amazon VPC Deploying Amazon EC2 Instances "},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/3.3-blog3/","title":"Securing VIN with Reference ID on AWS IoT","tags":[],"description":"","content":"Securing Vehicle Identification Number (VIN) with Reference ID in Connected Vehicle Platform on AWS IoT Introduction With over 470 million connected cars expected by the end of 2025, protecting sensitive vehicle data, especially Vehicle Identification Numbers (VIN), has become critically important for automotive manufacturers. VINs serve as unique identifiers throughout automotive processes, from manufacturing to maintenance, making them attractive targets for cybercriminals. The Problem VINs contain critical vehicle information (manufacturer, model, year) and can be linked to personal data. Unprotected VINs in cloud environments risk:\nIdentity theft Vehicle theft Insurance fraud Privacy violations Regulatory non-compliance (GDPR, CCPA) Solution: Reference ID This solution introduces Reference ID as a pseudonym for VIN, enabling secure vehicle data interaction without exposing actual VINs.\nHow It Works The system uses Reference ID, where each vehicle receives a unique identifier (UUID) during provisioning, acting as a VIN proxy in all platform interactions. The vehicle registry database stores both hashed and encrypted versions of the VIN, mapped to the Reference ID.\nVINs are encrypted using AWS Key Management Service (KMS) as a safeguard. When plain-text VIN retrieval is needed, this value can be decrypted, ensuring the actual VIN is only accessible when truly necessary.\nBenefits Acts as VIN proxy, enhancing security and data minimization Supports compliance with data protection regulations Provides flexible access control Improved auditability Scalability for large fleets Easier system interoperability Enables revocation without changing underlying VIN System Architecture 1. Reference ID Reference ID is a UUID generated during vehicle provisioning, serving as a VIN proxy throughout the vehicle\u0026rsquo;s lifecycle, creating an abstraction layer protecting sensitive VIN data.\n2. Vehicle Registry Database The vehicle registry database serves as a centralized repository for vehicle information. Key features:\nReference ID to hashed VIN mapping Encrypted VIN storage Vehicle provisioning and state change tracking Device change history Vehicle attributes and configuration Database Structure:\nreferenceId ‚Äì Partition key deviceId ‚Äì Global secondary index hashedVin ‚Äì Global secondary index tenantId encryptedVin 3. Vehicle Provisioning Process 3.1 Data Validation Provisioning infrastructure hashes VIN and queries database for first-time provisioning check For new vehicles, DEVICE_ID validated based on TCU manufacturer data Checks if DEVICE_ID already attached to another vehicle 3.2 Reference ID Generation Query database to validate if vehicle already provisioned If not, generate new UUID as Reference ID Store Reference ID, hashed VIN, and encrypted VIN (via KMS) Ensure UUID uniqueness 3.3 Certificate Generation Certificate generated using ACM PCA with Common Name = Reference ID 3.4 AWS IoT Integration Create AWS IoT Thing with Thing Name = Reference ID Create AWS IoT FleetWise Vehicle with Vehicle Name = Reference ID 3.5 Response Vehicle receives Certificate and Reference ID Vehicle connects to AWS IoT FleetWise using certificate and ClientId = ReferenceID 4. Data Collection and Storage 4.1 Vehicle to AWS IoT FleetWise Vehicle connects using Reference ID as Client ID All data associated with Reference ID 4.2 AWS IoT FleetWise to Data Platform Data enriched with Vehicle Name (Reference ID) 4.3 Storage and Retrieval Data stored with Reference ID as identifier Mobile app queries via API Platform using Reference ID 5. Customer Application Interaction 5.1 VIN to Reference ID Conversion After ownership verification, client app calls API for conversion API queries database to retrieve corresponding Reference ID Reference ID returned to application Security Considerations:\nStrict access control via authentication and authorization Log all conversion requests Rate limiting and DoS/DDoS protection Limit access to authorized applications 5.2 Using Reference ID After obtaining Reference ID, application can:\nRetrieve data from data platform Perform direct vehicle operations (remote commands) 6. Telematics Control Unit (TCU) Change 6.1 Update TCU Input: Hashed VIN (or Reference ID), current DEVICE_ID, new DEVICE_ID\nProcess:\nVerify hashed VIN and current DEVICE_ID Check new DEVICE_ID not linked to another vehicle Update DEVICE_ID in database Revoke and delete current certificate New TCU undergoes provisioning process 6.2 Remove TCU Input: Hashed VIN (or Reference ID), current DEVICE_ID\nProcess:\nVerify hashed VIN and DEVICE_ID Remove DEVICE_ID from database Revoke and delete certificate Security and Performance Considerations Security Minimizes VIN exposure risk in daily operations Only stores hashed and encrypted VINs AWS KMS encryption Strict access control policies Performance and Scalability Efficient UUID generation technology DynamoDB Global Secondary Indexes for fast queries Scalability for large fleets Future Blockchain or distributed ledger integration Advanced analytics and machine learning Continuous GDPR and CCPA compliance Conclusion The Reference ID system helps automotive manufacturers enhance VIN security in connected vehicle platforms on AWS. This architecture:\nProtects sensitive vehicle data Maintains full functionality Scales efficiently Meets compliance standards Provides flexible framework for vehicle identity management As connected vehicle numbers continue to grow, robust security measures become critically important. This system not only protects VINs but also helps meet data protection requirements.\nYou should explore how to apply this approach to your connected vehicle solutions. For more information about AWS IoT services and connected vehicle best practices, visit the AWS IoT FleetWise documentation and related blog posts.\nSource: AWS Blog\nDate: September 10, 2025\nTranslated by: Duong Nguyen Gia Huy\n"},{"uri":"https://thienluhoan.github.io/workshop-template/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":"This section will list and introduce the blogs you have translated:\nBlog 1 - AWS CDK Launches Refactoring Feature This blog introduces the new AWS Cloud Development Kit (CDK) refactoring feature that makes infrastructure as code refactoring easier and safer. You will learn how CDK Refactor preserves AWS resources when renaming constructs, moving resources between stacks, and reorganizing CDK applications - operations that previously risked resource replacement. The article demonstrates a practical example of refactoring a monolithic application into microservices architecture, showing how to split Lambda functions and DynamoDB tables across separate stacks while maintaining data integrity and avoiding downtime.\nBlog 2 - Modern Observability Frameworks for SAP This blog explores modern observability solutions for SAP environments using PowerConnect and Dynatrace on AWS. You will learn how to address monitoring challenges in complex SAP ecosystems that span SAP Cloud ERP, SAP BTP, and AWS services. The article explains how PowerConnect collects observability signals from SAP systems and integrates with Dynatrace\u0026rsquo;s AI-powered platform to provide comprehensive visibility, proactive issue detection, and automated root cause analysis. It includes detailed deployment architecture, pre-built dashboards, and integration with AWS AI services like Amazon Bedrock and Amazon Q.\nBlog 3 - Securing VIN with Reference ID on AWS IoT This blog presents a security solution for protecting Vehicle Identification Numbers (VIN) in connected vehicle platforms on AWS IoT. You will learn how to implement Reference ID as a pseudonym for VIN, enabling secure vehicle data interaction without exposing actual VINs. The article covers the complete architecture including vehicle provisioning process, database design with DynamoDB, certificate management with ACM PCA, integration with AWS IoT FleetWise, and handling TCU (Telematics Control Unit) changes. This approach helps automotive manufacturers comply with data protection regulations like GDPR and CCPA while maintaining full functionality.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.3-event3/","title":"AWS Cloud Mastery Series #3","tags":[],"description":"","content":"AWS Cloud Mastery Series #3 AWS Well-Architected Security Pillar Event Details üìÖ Saturday, November 29, 2025 (Morning Only)\nüï£ 8:30 AM ‚Äì 12:00 PM\nüìç Bitexco Financial Tower\nQu·∫≠n 1, Th√†nh ph·ªë H·ªì Ch√≠ Minh\nAgenda 8:30 ‚Äì 8:50 AM | Opening \u0026amp; Security Foundation Role of Security Pillar in Well-Architected Framework Core principles: Least Privilege ‚Äì Zero Trust ‚Äì Defense in Depth Shared Responsibility Model Top threats in cloud environments in Vietnam ‚≠ê Pillar 1 ‚Äî Identity \u0026amp; Access Management 8:50 ‚Äì 9:30 AM | Modern IAM Architecture IAM: Users, Roles, Policies ‚Äì avoiding long-term credentials IAM Identity Center: SSO, permission sets SCP \u0026amp; permission boundaries for multi-account MFA, credential rotation, Access Analyzer Mini Demo: Validate IAM Policy + simulate access ‚≠ê Pillar 2 ‚Äî Detection 9:30 ‚Äì 9:55 AM | Detection \u0026amp; Continuous Monitoring CloudTrail (org-level), GuardDuty, Security Hub Logging at every layer: VPC Flow Logs, ALB/S3 logs Alerting \u0026amp; automation with EventBridge Detection-as-Code (infrastructure + rules) 9:55 ‚Äì 10:10 AM | Coffee Break ‚≠ê Pillar 3 ‚Äî Infrastructure Protection 10:10 ‚Äì 10:40 AM | Network \u0026amp; Workload Security VPC segmentation, private vs public placement Security Groups vs NACLs: application models WAF + Shield + Network Firewall Workload protection: EC2, ECS/EKS security basics ‚≠ê Pillar 4 ‚Äî Data Protection 10:40 ‚Äì 11:10 AM | Encryption, Keys \u0026amp; Secrets KMS: key policies, grants, rotation Encryption at-rest \u0026amp; in-transit: S3, EBS, RDS, DynamoDB Secrets Manager \u0026amp; Parameter Store ‚Äî rotation patterns Data classification \u0026amp; access guardrails ‚≠ê Pillar 5 ‚Äî Incident Response 11:10 ‚Äì 11:40 AM | IR Playbook \u0026amp; Automation IR lifecycle according to AWS Playbooks: Compromised IAM key S3 public exposure EC2 malware detection Snapshot, isolation, evidence collection Auto-response using Lambda/Step Functions 11:40 ‚Äì 12:00 PM | Wrap-Up \u0026amp; Q\u0026amp;A Summary of 5 pillars Common pitfalls \u0026amp; real-world scenarios in Vietnamese enterprises Security learning roadmap (Security Specialty, SA Pro) Key Takeaways Comprehensive understanding of AWS Well-Architected Security Pillar Hands-on IAM best practices and policy validation Detection and monitoring strategies with AWS security services Infrastructure and data protection implementation Incident response automation and playbooks Security certification pathways "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.3-connect-data/","title":"Connect Data","tags":[],"description":"","content":"CONNECT DATA Step 1: New Dataset In the QuickSight console, choose Datasets from the left navigation pane. Choose New dataset. Under FROM NEW DATA SOURCES, choose Upload a file. Select the sales_data.csv file you created in the previous step. Step 2: Preview and Edit After the file uploads, QuickSight will show a preview. Choose Edit settings and prepare data. Here you can see the columns: Date, Region, Product, Sales, Quantity. Ensure Sales and Quantity are detected as numbers (Integers or Decimals). Ensure Date is detected as a Date type. Choose Save \u0026amp; Publish at the top right. Choose Cancel to go back to the main screen, or directly click Visualize if prompted. "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Master Amazon EC2 and its basic features. Understand EC2 instance types, AMI, and pricing models. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon EC2 Fundamentals + EC2 instance types (General Purpose, Compute Optimized, Memory Optimized) + AMI (Amazon Machine Images) + Key pairs and Security Groups 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - EC2 Hands-on: + Launch EC2 instances (Linux and Windows) + Connect via SSH/RDP + Configure Security Groups + Use User Data scripts 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn EC2 Advanced Features + Launch Templates + Create custom AMI + EC2 Instance Metadata + Placement Groups 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Deploy applications on EC2 + Deploy web application on Amazon Linux + Deploy application on Windows Server 2022 + Configure basic Load Balancer 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn EC2 Pricing Models + On-Demand, Reserved, Spot Instances + Savings Plans + Cost optimization strategies + Clean up resources 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Mastered Amazon EC2:\nEC2 instance types and use cases AMI (Amazon Machine Images) and how to create custom AMI Key pairs and Security Groups EC2 User Data for bootstrapping instances EC2 Instance Metadata service Understood EC2 Advanced Features:\nLaunch Templates for standardizing deployments Placement Groups (Cluster, Spread, Partition) EC2 Instance Connect Elastic IP addresses Successfully practiced:\nLaunch and connect to EC2 instances (Linux and Windows) Configure Security Groups and Network ACLs Create custom AMI from running instance Deploy web applications on EC2 Use User Data scripts to automate setup Understood EC2 Pricing Models:\nOn-Demand Instances (pay as you go) Reserved Instances (1 or 3 years) Spot Instances (cheap but can be terminated) Savings Plans Cost optimization best practices "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.4-event4/","title":"AI-Driven Development Workshop","tags":[],"description":"","content":"AI-Driven Development Workshop Amazon Q Developer \u0026amp; Kiro IDE Event Details üìÖ Friday, October 3, 2025\nüïë 2:00 PM ‚Äì 4:30 PM\nüìç AWS Event Hall, L26 Bitexco Tower, HCMC\nInstructors: Toan Huynh \u0026amp; My Nguyen\nCoordinators: Diem My, Dai Truong, Dinh Nguyen\nAgenda 2:00 PM - 2:15 PM | Welcoming Introduction to AI-Driven Development Workshop overview and objectives Instructor and participant introductions 2:15 PM - 3:30 PM | AI-Driven Development Life Cycle Presented by: Toan Huynh\nAI-Driven Development Life Cycle Overview\nModern software development with AI assistance Benefits and productivity gains Integration into existing workflows Amazon Q Developer Demonstration\nCode generation and completion Code explanation and documentation Bug detection and security scanning Test generation Code refactoring and optimization Multi-language support Live coding demonstration 3:30 PM - 3:45 PM | Break 3:45 PM - 4:30 PM | Kiro IDE Demonstration Presented by: My Nguyen\nKiro IDE Overview\nAI-powered development environment Intelligent code assistance Context-aware suggestions Integrated AI chat and code generation Live Demonstration\nSetting up Kiro IDE AI-assisted coding workflows Real-world development scenarios Best practices and tips Key Takeaways Understanding of AI-Driven Development lifecycle Hands-on experience with Amazon Q Developer Introduction to Kiro IDE capabilities Practical knowledge of AI-assisted coding tools Productivity enhancement strategies Integration of AI tools into daily development workflow "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.4-create-visuals/","title":"Create Visuals","tags":[],"description":"","content":"CREATE VISUALS Step 1: Create an Analysis If you are not already in the analysis view, go to Datasets, select your dataset, and choose Create analysis. You will see an empty workspace called a \u0026ldquo;Sheet\u0026rdquo;. Step 2: Visual 1 - Sales by Product (Donut Chart) In the Visual types pane (bottom left), select the Donut chart icon. From the Fields list (left), drag Product to the Group/Color well. Drag Sales to the Value well. You should see a donut chart showing sales distribution by product. Step 3: Visual 2 - Sales Trend over Time (Line Chart) Choose Add \u0026gt; Add visual from the top menu bar. Select the Line chart icon. Drag Date to the X axis well. Drag Sales to the Value well. You should see a line chart showing sales over time. Step 4: Visual 3 - Sales by Region (Bar Chart) Choose Add \u0026gt; Add visual. Select the Vertical bar chart icon. Drag Region to the X axis well. Drag Sales to the Value well. You can now see which region performs best. "},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Events Participated During the internship period, I actively participated in various AWS events and workshops to enhance my cloud computing knowledge and networking with industry professionals.\nEvent Summary 1. AWS Cloud Mastery Series #1 - AI/ML/GenAI on AWS üìÖ November 15, 2025 | üï£ 8:30 AM ‚Äì 12:00 PM\nKey Topics:\nAmazon SageMaker end-to-end ML platform Generative AI with Amazon Bedrock Foundation Models (Claude, Llama, Titan) RAG (Retrieval-Augmented Generation) architecture Skills Gained: ML model deployment, prompt engineering, RAG implementation\n2. AWS Cloud Mastery Series #2 - DevOps on AWS üìÖ November 17, 2025 | üï£ 8:30 AM ‚Äì 5:00 PM\nKey Topics:\nCI/CD Pipeline with CodeCommit, CodeBuild, CodeDeploy, CodePipeline Infrastructure as Code (CloudFormation \u0026amp; CDK) Container Services (ECS, EKS, ECR) Monitoring \u0026amp; Observability (CloudWatch, X-Ray) Skills Gained: CI/CD automation, IaC implementation, container orchestration\n3. AWS Cloud Mastery Series #3 - Security Pillar üìÖ November 29, 2025 | üï£ 8:30 AM ‚Äì 12:00 PM\nKey Topics:\n5 Security Pillars: IAM, Detection, Infrastructure Protection, Data Protection, Incident Response Modern IAM architecture with Identity Center Security services: GuardDuty, Security Hub, CloudTrail Encryption with KMS and Secrets Manager Skills Gained: Security best practices, IAM policy design, incident response automation\n4. AI-Driven Development Workshop üìÖ October 3, 2025 | üïë 2:00 PM ‚Äì 4:30 PM\nKey Topics:\nAI-Driven Development Life Cycle Amazon Q Developer demonstration Kiro IDE capabilities and workflows Skills Gained: AI-assisted coding, productivity enhancement with AI tools\n5. AWS GenAI Summit üìÖ TBD | üï£ 8:30 AM ‚Äì 12:00 PM\nKey Topics:\nEnterprise Chatbot with MCP on AWS Stock Trading Chatbot for FinTech GenAI Recipe Recommendation System Scaling eCommerce with GenAI Internal Chatbot with RAG on AWS Serverless AI-powered Auto Audit Framework Multi-Agent Systems for Process Automation Cloud-native Architecture Solutions (NovaAct) GenAI with Kiro IDE \u0026amp; Strands Agent Skills Gained: Multi-agent systems, industry-specific GenAI applications, MCP integration\nOverall Impact Total Events Attended: 5\nTotal Hours: ~30 hours\nKey Technologies Learned: AWS Bedrock, SageMaker, DevOps tools, Security services, GenAI frameworks\nProfessional Development:\nEnhanced understanding of AWS services and architecture Practical experience with cutting-edge AI/ML technologies Networking with AWS professionals and community members Exposure to real-world use cases and best practices Career pathway insights and certification guidance "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Master AWS IAM for access management and security. Understand EC2 Instance Storage and storage types. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS IAM + Users, Groups, Roles + Policies and Permissions + IAM best practices 09/22/2025 09/22/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice IAM: + Create users and groups + Assign policies + Create and use IAM roles 09/23/2025 09/23/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn EC2 Instance Storage + Amazon EBS (Elastic Block Store) + EC2 Instance Store + Amazon EFS (Elastic File System) 09/24/2025 09/24/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice Storage: + Create and attach EBS volumes + Create snapshots + Use EFS for shared storage 09/25/2025 09/25/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about backup and disaster recovery + EBS snapshots + AMI creation + Cross-region backup 09/26/2025 09/26/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Mastered AWS IAM concepts:\nUsers, Groups, Roles Policies and permission assignment MFA (Multi-Factor Authentication) IAM security best practices Understood EC2 Instance Storage:\nEBS volumes and types (gp3, io2, st1, sc1) EC2 Instance Store (ephemeral storage) EFS for shared file systems Comparison between storage types Successfully practiced:\nCreating and managing IAM users, groups, roles Creating, attaching and managing EBS volumes Creating snapshots and restoring Setting up EFS and mounting to EC2 Understood backup strategies and disaster recovery for EC2 and storage.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/4-eventparticipated/4.5-event5/","title":"AWS GenAI Summit","tags":[],"description":"","content":"AWS GenAI Summit Generative AI Solutions \u0026amp; Use Cases Event Details üìÖ Date: TBD\nüï£ 8:30 AM ‚Äì 12:00 PM\nüìç AWS Vietnam Office\nOpening Speaker: Nguy·ªÖn Gia H∆∞ng, Head of Solutions Architect, AWS\nAgenda 8:30 ‚Äì 9:00 AM | Welcome \u0026amp; Coffee Participant registration and networking Welcome coffee and refreshments 9:00 ‚Äì 9:30 AM | Opening Keynote Speaker: Nguy·ªÖn Gia H∆∞ng, Head of Solutions Architect, AWS\nState of Generative AI in Vietnam AWS GenAI services overview Industry trends and opportunities 9:30 ‚Äì 12:00 PM | Parallel Sessions Multiple tracks featuring cutting-edge GenAI solutions:\nTrack 1: Enterprise \u0026amp; Business Solutions Enterprise Chatbot v·ªõi MCP tr√™n AWS\nModel Context Protocol integration Enterprise-grade chatbot architecture Scalability and security considerations Internal Chatbot with RAG on AWS Serverless\nRetrieval-Augmented Generation implementation Serverless architecture patterns Cost optimization strategies AI-powered Auto Audit Framework\nAutomated compliance and audit systems AI-driven anomaly detection Regulatory compliance automation Track 2: Industry-Specific Applications Stock Trading Chatbot cho FinTech\nReal-time market data integration AI-powered trading insights Risk management and compliance GenAI Recipe Recommendation System\nPersonalized recommendation engines Natural language recipe generation User preference learning Scaling eCommerce with GenAI\nProduct description generation Customer service automation Personalized shopping experiences Track 3: Advanced AI Systems Multi-Agent Systems for Process Automation\nAgent orchestration patterns Complex workflow automation Inter-agent communication Cloud-native Architecture Solutions (NovaAct)\nModern cloud architecture with GenAI Infrastructure automation Best practices and patterns GenAI with Kiro IDE \u0026amp; Strands Agent\nAI-assisted development workflows Agent-based coding assistance Developer productivity enhancement Key Takeaways Comprehensive overview of GenAI applications across industries Hands-on insights from real-world implementations Best practices for building GenAI solutions on AWS Networking with GenAI practitioners and AWS experts Understanding of MCP, RAG, and multi-agent systems Industry-specific use cases and success stories "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.5-publish-dashboard/","title":"Publish Dashboard","tags":[],"description":"","content":"PUBLISH DASHBOARD Step 1: Arrange Visuals You can resize and move the visuals on the sheet to make them look organized. Click on the title of each visual to rename it (e.g., \u0026ldquo;Sales by Product\u0026rdquo;). Add a title to the sheet at the top (e.g., \u0026ldquo;Sales Overview Dashboard\u0026rdquo;). Step 2: Publish Dashboard At the top right, choose Share \u0026gt; Publish dashboard. Publish new dashboard as: Enter a name, e.g., Sales-Dashboard-v1. Choose Publish dashboard. You can now share this dashboard with other users in your QuickSight account or share it via a link. Congratulations! You have successfully created a Business Intelligence dashboard using Amazon QuickSight.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Understand High Availability and Scalability in AWS. Master database services: RDS, Aurora, ElastiCache. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn High Availability \u0026amp; Scalability + Multi-AZ deployments + Auto Scaling Groups + Elastic Load Balancer (ALB, NLB, CLB) 09/29/2025 09/29/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice HA \u0026amp; Scalability: + Create Auto Scaling Group + Configure Load Balancer + Test scaling policies 09/30/2025 09/30/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Amazon RDS + RDS engines (MySQL, PostgreSQL, Oracle, SQL Server) + Multi-AZ and Read Replicas + Backup and restore 10/01/2025 10/01/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn Amazon Aurora \u0026amp; ElastiCache + Aurora MySQL/PostgreSQL + Aurora Serverless + ElastiCache (Redis, Memcached) 10/02/2025 10/02/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice Database: + Create RDS instance + Configure Multi-AZ + Create Read Replica + Use ElastiCache 10/03/2025 10/03/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Mastered High Availability and Scalability concepts:\nMulti-AZ deployments Auto Scaling Groups and scaling policies Load Balancing (ALB, NLB, CLB) Health checks and monitoring Understood AWS database services:\nAmazon RDS and database engines Multi-AZ for high availability Read Replicas for read scalability Amazon Aurora and Aurora Serverless ElastiCache for caching (Redis, Memcached) Successfully practiced:\nSetting up Auto Scaling Group with Launch Template Configuring Application Load Balancer Creating and managing RDS instances Configuring Multi-AZ and Read Replicas Using ElastiCache to improve performance Understood database backup, restore and disaster recovery strategies.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/","title":"Workshop Business Intelligence with Amazon QuickSight","tags":[],"description":"","content":"Overview In this workshop, you will learn how to use Amazon QuickSight, a fast, cloud-powered business intelligence service that makes it easy to deliver insights to everyone in your organization.\nYou will sign up for Amazon QuickSight, connect to a data source, create an analysis with various visualizations, and finally publish a dashboard.\nContent Workshop overview Prerequiste Connect Data Create Visuals Publish Dashboard Clean up "},{"uri":"https://thienluhoan.github.io/workshop-template/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Clean up resources To avoid incurring future charges, you should unsubscribe from QuickSight if you do not plan to use it further.\nManage QuickSight: Click on your user icon in the top right corner. Choose Manage QuickSight. Unsubscribe from QuickSight: Go to Account settings. Choose Delete account (this unsubscribes you from QuickSight). Type delete to confirm. Choose Delete account. Note: If you are on the Free Trial, you will not be charged until the trial ends, but it is good practice to clean up if you are done.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at Amazon Web Services Vietnam from 08/09/2025 to 12/12/2025, I had the opportunity to learn, practice, and apply AI/ML knowledge in a real-world environment working with cutting-edge AWS services.\nI participated in developing a RAG (Retrieval-Augmented Generation) system using Amazon Bedrock. Through this project, I significantly improved my skills in: Python programming, serverless architecture, AI/ML engineering, natural language processing, and working with AWS services.\nIn terms of work ethic, I always strived to complete tasks well, proactively learned new technologies, and actively engaged with the team to improve work efficiency.\nTo objectively reflect on my internship period, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of AI/ML, AWS services, applying knowledge to RAG pipeline, backend development skills ‚úÖ ‚òê ‚òê 2 Ability to learn Quickly absorbing new technologies (Bedrock, RAG, embeddings), self-learning through documentation ‚úÖ ‚òê ‚òê 3 Proactiveness Researching RAG, proposing chunking and embedding solutions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing Lambda modules and integrations on timeline, ensuring code quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, code review processes, AWS ‚òê ‚úÖ ‚òê 6 Progressive mindset Willingness to receive code feedback, improving RAG pipeline performance ‚úÖ ‚òê ‚òê 7 Communication Presenting technical designs, reporting progress and issues clearly ‚òê ‚úÖ ‚òê 8 Teamwork Working effectively with team, integrating code with other components ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, following security and compliance guidelines ‚úÖ ‚òê ‚òê 10 Problem-solving skills Debugging embedding issues, optimizing chunking strategy, troubleshooting Lambda errors ‚úÖ ‚òê ‚òê 11 Contribution to project/team Successfully building RAG pipeline, contributing to knowledge base architecture ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Strengths Technical skills: Strong grasp of RAG architecture, Bedrock integration, and embeddings processing Problem-solving: Capable of debugging and optimizing AI pipeline performance Proactive learning: Self-researching and applying best practices for document chunking and retrieval Teamwork: Effective collaboration with team in integrating various components Needs Improvement Discipline and processes: Need to more strictly follow code review and testing procedures Communication skills: Improve ability to present technical concepts to non-technical stakeholders Production knowledge: Learn more about monitoring, logging, and troubleshooting in production environments Cost optimization: Learn how to optimize AWS costs when working with Bedrock and Lambda "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Master Amazon Route 53 and DNS routing. Understand Classic Solutions Architecture and design patterns. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon Route 53 + DNS fundamentals + Hosted zones + Routing policies (Simple, Weighted, Latency, Failover, Geolocation) 10/06/2025 10/06/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice Route 53: + Register domain or use existing domain + Configure routing policies + Health checks 10/07/2025 10/07/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Classic Solutions Architecture + 3-tier architecture + Stateless web tier + Stateful application tier 10/08/2025 10/08/2025 https://cloudjourney.awsstudygroup.com/ 5 - Design sample architectures + WordPress on AWS + E-commerce platform + Microservices architecture 10/09/2025 10/09/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Draw architecture with draw.io + Deploy a simple 3-tier architecture + Document architecture decisions 10/10/2025 10/10/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Mastered Amazon Route 53:\nDNS fundamentals and how it works Hosted zones (public and private) Routing policies and use cases Health checks and DNS failover Domain registration Understood Classic Solutions Architecture:\n3-tier architecture (Web, App, Database) Stateless vs Stateful design Horizontal vs Vertical scaling Best practices for high availability Cost optimization strategies Successfully practiced:\nConfiguring Route 53 with routing policies Setting up health checks and failover Drawing AWS architecture with draw.io Deploying a simple 3-tier architecture Documenting and presenting architecture decisions Able to design and deploy basic architectures on AWS.\n"},{"uri":"https://thienluhoan.github.io/workshop-template/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program. This will help the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe working environment at AWS is highly professional and dynamic. The team is always ready to help when I encounter technical issues. The workspace is well-equipped with AWS tools and resources for experimentation and development.\n2. Support from Mentor / Team Admin\nMy mentor has deep knowledge of AWS services, guiding me from overall architecture. Importantly, the mentor always encourages me to research and propose solutions before asking, which helps develop independent thinking. The admin team provides excellent support with AWS account setup and permissions.\n3. Relevance of Work to Academic Major\nThe Document QA project with AWS Bedrock aligns perfectly with my AI major. I applied knowledge of NLP, embeddings, and machine learning in practice. Additionally, I learned about cloud architecture, serverless computing, and production-grade AI systems - crucial knowledge not covered in detail at university.\n4. Learning \u0026amp; Skill Development Opportunities\nParticipated in 10 weeks of workshops covering AWS services, from basic to advanced. Especially attended 5 high-quality AWS events (Vietnam Cloud Day, Cloud Mastery Series), learning directly from Solutions Architects and experts. My mentor shared extensive real-world experience that helped me plan my career path in Cloud Computing.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. Networking with the AWS community in Vietnam through events. This made me feel like part of a larger cloud community, not just an intern.\n6. Internship Policies / Benefits\nFree participation in AWS events, flexible working hours, learning environment with full resources. Could consider providing additional AWS credits for interns to practice.\nAdditional Questions What did you find most satisfying during your internship?\nAttending 5 high-quality AWS events, learning directly from experts. Completing 10 weeks of workshops helped me gain a comprehensive view of the AWS ecosystem.\nWhat do you think the company should improve for future interns?\nProvide a checklist of skills to achieve after each phase Provide specific learning paths for each technical domain (AI/ML, Backend, DevOps\u0026hellip;) If recommending to a friend, would you suggest they intern here? Why or why not?\nAbsolutely! Dedicated mentors with high expertise, and opportunities to attend quality AWS events.\nSuggestions \u0026amp; Expectations Suggestions to improve the internship experience:\nCreate a small \u0026ldquo;starter project\u0026rdquo; for interns to familiarize themselves with AWS services before joining the main project Would you like to continue this program in the future?\nYes, I would like to continue working with AWS, especially in the AI/ML field.\nAny other comments:\nThank you to the FCJ team and AWS for creating this wonderful opportunity. I learned so much not only about technical skills but also about working professionally in a world-leading tech company. I hope the program continues to grow and creates opportunities for more Vietnamese students!\n"},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Master Amazon S3 and storage features. Understand S3 security and advanced features. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Amazon S3 Basics + Buckets and Objects + Storage classes (Standard, IA, Glacier) + Versioning 10/13/2025 10/13/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn S3 Advanced Features + Lifecycle policies + Cross-region replication + S3 Transfer Acceleration + S3 Select 10/14/2025 10/14/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn S3 Security + Bucket policies + IAM policies for S3 + Encryption (SSE-S3, SSE-KMS, SSE-C) + Access Control Lists (ACLs) 10/15/2025 10/15/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice S3: + Create buckets and upload objects + Configure versioning + Set up lifecycle policies 10/16/2025 10/16/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice S3 Security: + Configure bucket policies + Enable encryption + Setup cross-region replication 10/17/2025 10/17/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Mastered Amazon S3:\nBuckets, objects and S3 fundamentals Storage classes and cost optimization Versioning and object lifecycle S3 performance optimization Understood S3 Advanced Features:\nLifecycle policies to automatically transition storage classes Cross-region replication for disaster recovery S3 Transfer Acceleration S3 Select and Glacier Select Mastered S3 Security:\nBucket policies and IAM policies Encryption options (SSE-S3, SSE-KMS, SSE-C) Access Control Lists (ACLs) S3 Block Public Access Pre-signed URLs Successfully practiced:\nCreating and managing S3 buckets Configuring versioning and lifecycle policies Setting up encryption and bucket policies Setting up cross-region replication Hosting static website on S3 "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Understand CloudFront, Global Accelerator and AWS Storage Extras. Master AWS Integration \u0026amp; Messaging services. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn CloudFront \u0026amp; Global Accelerator + CloudFront distributions + Origins and behaviors + Global Accelerator use cases 10/20/2025 10/20/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice CloudFront: + Create CloudFront distribution + Configure S3 origin + Custom domain with SSL/TLS 10/21/2025 10/21/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn AWS Storage Extras + AWS Storage Gateway + FSx for Windows File Server + FSx for Lustre + AWS Backup 10/22/2025 10/22/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn AWS Integration \u0026amp; Messaging + Amazon SQS (Standard, FIFO) + Amazon SNS + Amazon Kinesis + AWS Step Functions 10/23/2025 10/23/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice Messaging: + Create SQS queues + Create SNS topics + Connect SQS with SNS + Create Step Functions workflow 10/24/2025 10/24/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Mastered CloudFront and Global Accelerator:\nCloudFront distributions and caching strategies Origins (S3, EC2, ALB, custom origins) Cache behaviors and TTL Global Accelerator for low latency Comparison: CloudFront vs Global Accelerator Understood AWS Storage Extras:\nAWS Storage Gateway (File, Volume, Tape) FSx for Windows File Server FSx for Lustre for HPC workloads AWS Backup for centralized backup Mastered AWS Integration \u0026amp; Messaging:\nAmazon SQS (Standard vs FIFO queues) Amazon SNS (pub/sub messaging) Amazon Kinesis (Data Streams, Firehose, Analytics) AWS Step Functions (workflow orchestration) EventBridge for event-driven architecture Successfully practiced:\nCreating and configuring CloudFront distribution Setting up custom domain with SSL/TLS Creating SQS queues and SNS topics Connecting messaging services Creating Step Functions state machine "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Master Containers and Serverless on AWS. Understand serverless architectures and best practices. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Containers on AWS + Docker fundamentals + Amazon ECS (Elastic Container Service) + Amazon EKS (Elastic Kubernetes Service) 10/27/2025 10/27/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice Containers: + Create Docker image + Push image to ECR + Deploy container on ECS + Fargate vs EC2 launch types 10/28/2025 10/28/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Serverless Overview + AWS Lambda fundamentals + API Gateway + DynamoDB + Lambda triggers and integrations 10/29/2025 10/29/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn Serverless Architectures + Serverless web application + Event-driven architecture + Lambda best practices + SAM (Serverless Application Model) 10/30/2025 10/30/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice Serverless: + Create Lambda functions + Configure API Gateway + Connect with DynamoDB + Deploy serverless app 10/31/2025 10/31/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Mastered Containers on AWS:\nDocker fundamentals and containerization Amazon ECS (Fargate and EC2 launch types) Amazon EKS for Kubernetes workloads Amazon ECR (Elastic Container Registry) Task definitions and services Understood Serverless:\nAWS Lambda and event-driven computing Lambda triggers (S3, DynamoDB, API Gateway, etc.) Lambda layers and environment variables API Gateway (REST and HTTP APIs) DynamoDB for serverless databases Mastered Serverless Architectures:\nServerless web applications Event-driven architectures Lambda best practices (cold starts, memory, timeout) AWS SAM (Serverless Application Model) Serverless Framework Successfully practiced:\nBuilding and deploying Docker containers on ECS Creating and deploying Lambda functions Configuring API Gateway with Lambda Creating serverless CRUD application with DynamoDB Deploying serverless app with SAM/CloudFormation "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Master Databases, Data \u0026amp; Analytics services. Understand Machine Learning services on AWS. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Databases in AWS + DynamoDB (NoSQL) + Amazon Redshift (Data Warehouse) + Neptune, DocumentDB, Timestream 11/03/2025 11/03/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice Databases: + Create DynamoDB table + Query and scan operations + DynamoDB Streams + Global Tables 11/04/2025 11/04/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Data \u0026amp; Analytics + Amazon Athena + AWS Glue + Amazon EMR + Amazon Kinesis + QuickSight 11/05/2025 11/05/2025 https://cloudjourney.awsstudygroup.com/ 5 - Practice Data \u0026amp; Analytics: + Query S3 data with Athena + Create Glue crawler + Setup Kinesis stream 11/06/2025 11/06/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn Machine Learning + Amazon SageMaker + Rekognition, Comprehend + Translate, Polly, Transcribe + Amazon Lex 11/07/2025 11/07/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Mastered Databases in AWS:\nDynamoDB (NoSQL database) DynamoDB Streams and Global Tables Amazon Redshift (Data Warehouse) Amazon Neptune (Graph Database) Amazon DocumentDB (MongoDB compatible) Amazon Timestream (Time series database) Understood Data \u0026amp; Analytics:\nAmazon Athena (serverless query service) AWS Glue (ETL service) Amazon EMR (Elastic MapReduce) Amazon Kinesis (real-time data streaming) AWS Data Pipeline Amazon QuickSight (BI service) Mastered Machine Learning services:\nAmazon SageMaker (build, train, deploy ML models) Amazon Rekognition (image and video analysis) Amazon Comprehend (NLP service) Amazon Translate, Polly, Transcribe Amazon Lex (chatbots) Successfully practiced:\nCreating and querying DynamoDB tables Using Athena to query S3 data Setting up Glue crawler and ETL jobs Creating Kinesis stream for real-time data Using SageMaker and Rekognition "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Master AWS Monitoring, Security and Advanced Identity. Understand AWS VPC and networking. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn AWS Monitoring \u0026amp; Performance + Amazon CloudWatch (metrics, logs, alarms) + AWS CloudTrail + AWS Config + AWS Trusted Advisor 11/10/2025 11/10/2025 https://cloudjourney.awsstudygroup.com/ 3 - Practice Monitoring: + Create CloudWatch dashboards + Setup alarms and notifications + Enable CloudTrail + Review Trusted Advisor 11/11/2025 11/11/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn Advanced Identity \u0026amp; Security + AWS Organizations + AWS SSO + AWS KMS (Key Management Service) + CloudHSM, Secrets Manager 11/12/2025 11/12/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn AWS Security Services + AWS Shield \u0026amp; WAF + Amazon GuardDuty + Amazon Inspector + AWS Security Hub 11/13/2025 11/13/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn Amazon VPC + Subnets, Route Tables, Internet Gateway + NAT Gateway, VPN + VPC Peering, Transit Gateway + VPC Endpoints 11/14/2025 11/14/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Mastered AWS Monitoring \u0026amp; Performance:\nAmazon CloudWatch (metrics, logs, alarms, dashboards) AWS CloudTrail (audit and compliance) AWS Config (resource inventory and compliance) AWS Trusted Advisor (best practices recommendations) AWS X-Ray (distributed tracing) Understood Advanced Identity \u0026amp; Security:\nAWS Organizations (multi-account management) AWS SSO (Single Sign-On) AWS KMS (Key Management Service) AWS CloudHSM AWS Secrets Manager and Parameter Store AWS Certificate Manager Mastered AWS Security Services:\nAWS Shield (DDoS protection) AWS WAF (Web Application Firewall) Amazon GuardDuty (threat detection) Amazon Inspector (vulnerability assessment) AWS Security Hub (centralized security) Understood Amazon VPC:\nVPC components (subnets, route tables, IGW, NAT) Security Groups and NACLs VPC Peering and Transit Gateway VPN and Direct Connect VPC Endpoints (Gateway and Interface) Successfully practiced:\nSetting up CloudWatch monitoring and alarms Enabling CloudTrail and Config Configuring KMS encryption Creating and managing VPC with public/private subnets Setting up VPN connection "},{"uri":"https://thienluhoan.github.io/workshop-template/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Master Disaster Recovery and Migration strategies. Review comprehensively and prepare for AWS Solutions Architect Associate. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn Disaster Recovery \u0026amp; Migrations + DR strategies (Backup \u0026amp; Restore, Pilot Light, Warm Standby, Multi-Site) + AWS Backup + AWS DRS (Disaster Recovery Service) 11/17/2025 11/17/2025 https://cloudjourney.awsstudygroup.com/ 3 - Learn AWS Migration Services + AWS Application Discovery Service + AWS Migration Hub + AWS Database Migration Service (DMS) + AWS Server Migration Service (SMS) 11/18/2025 11/18/2025 https://cloudjourney.awsstudygroup.com/ 4 - Review More Solutions Architecture + Hybrid cloud architectures + Multi-region architectures + Event-driven architectures + Well-Architected Framework 11/19/2025 11/19/2025 https://cloudjourney.awsstudygroup.com/ 5 - Read AWS White Papers \u0026amp; Best Practices + AWS Well-Architected Framework + Security Best Practices + Cost Optimization + Reliability Pillar 11/20/2025 11/20/2025 https://cloudjourney.awsstudygroup.com/ 6 - Comprehensive review and practice tests: + Review all modules + Take practice exams + Complete final workshop 11/21/2025 11/21/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Mastered Disaster Recovery strategies:\nBackup \u0026amp; Restore (high RPO/RTO, low cost) Pilot Light (medium RPO/RTO) Warm Standby (low RPO/RTO) Multi-Site/Hot Site (very low RPO/RTO, high cost) AWS Backup and AWS Elastic Disaster Recovery Understood AWS Migration:\nAWS Application Discovery Service AWS Migration Hub AWS Database Migration Service (DMS) AWS Server Migration Service (SMS) AWS DataSync and Transfer Family 6 R\u0026rsquo;s of Migration (Rehost, Replatform, Repurchase, Refactor, Retire, Retain) Mastered More Solutions Architecture:\nHybrid cloud architectures Multi-region architectures Event-driven architectures Microservices patterns AWS Well-Architected Framework (5 pillars) Completed AWS learning path:\nStudied and practiced 29 modules Understood main AWS services Able to design AWS architectures Ready for AWS Solutions Architect Associate exam Completed final workshop and submitted via Drive/GitHub "},{"uri":"https://thienluhoan.github.io/workshop-template/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://thienluhoan.github.io/workshop-template/tags/","title":"Tags","tags":[],"description":"","content":""}]